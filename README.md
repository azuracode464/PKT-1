
PKT-1: Lightweight Text Language Model

PKT-1 is a lightweight text-based language model designed for efficient training and inference, optimized for medium-sized datasets and limited hardware environments like cloud-based Codespaces. Built with performance and memory efficiency in mind, PKT-1 allows experimentation with billion-parameter models using float16 precision.


---

‚ö° Key Features

Model Size: 1.5 billion parameters (float16)

Dataset: Suitable for 100 MB text datasets (~20M tokens)

Batch Size: Optimized for batch 16

Precision: Float16 for memory efficiency

Checkpointing: Supports frequent checkpoints to avoid losing progress

Hardware Friendly: Can be trained on 16 GB RAM environments without crashing



---

üõ†Ô∏è Requirements

Python 3.10+

PyTorch (ARM / CPU or GPU compatible)

Hugging Face Transformers (optional, for tokenizer and training utilities)

16 GB RAM recommended for smooth training with 1.5B parameters



---

üöÄ Training

Example Python snippet:

from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_config({
    "n_embd": 2048,
    "n_layer": 24,
    "n_head": 16,
    "vocab_size": tokenizer.vocab_size,
    "dtype": "float16"
})

dataset = load_your_dataset("path/to/100MB_dataset.txt")

training_args = TrainingArguments(
    output_dir="./pkt1-checkpoints",
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,
    fp16=True,
    save_steps=500,
    logging_steps=100,
    num_train_epochs=3
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

trainer.train()


---

üì¶ Model Usage

After training, you can use PKT-1 for:

Text generation

Chatbots / conversational agents

Fine-tuning on custom domains

Experimenting with lightweight AI models on cloud environments



---

üìù Notes

PKT-1 is designed for experimentation and educational purposes.

Adjust batch size and gradient accumulation depending on your environment.

For larger datasets or models, consider using GPUs or cloud environments with more RAM.



---

‚ö° License

MIT License ‚Äì free to use, modify, and distribute.

